{
  "nbformat": 4,
  "nbformat_minor": 0,
  "metadata": {
    "colab": {
      "name": "Neural network NER.ipynb",
      "provenance": [],
      "collapsed_sections": [
        "yPNWhcjo83aQ",
        "vjJXeAqUB0Pi",
        "mntF9f09fyGH",
        "MQRwbF8RBjbL",
        "hnwM4TAKcQkq",
        "QZ-8aXT4Bf6r",
        "d6H0xHZ3BtVL"
      ],
      "toc_visible": true
    },
    "kernelspec": {
      "display_name": "Python 3",
      "name": "python3"
    }
  },
  "cells": [
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "rMskbkllpf3g"
      },
      "source": [
        "# NER with Neural Networks\n",
        "\n",
        "\n",
        "\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QcG22Tt2pk32"
      },
      "source": [
        "# Model description\n",
        "\n",
        "The purpose of this work is to build a neurak-network classifier for Named-Entity Recognition. This work is extracted from the W-NUT 2017 Shared Task, that challenged competitors with unusual and previously unseen named-entities. To do so, data come from user-generated texts from Twitter, YouTube, Reddit and StackExchange.\n",
        "\n",
        "In this work, we explore two approaches to improve the performance of a simple neural network. In the first one, we compare the impact on performance of the tagging scheme used to tag named-entities. In the second one, we examine which word embedding representation achieves best results. \n",
        "\n",
        "The following code consists of three parts:\n",
        "  - data processing: data are pre-processed so that they can be used as input of our neural network:\n",
        "      1. NER labels are converted into the tagging scheme chosen\n",
        "      2. word tokens and NER labels are encoded as integer values\n",
        "      3. tokens are converted to lower case\n",
        "      4. data are transformed into padded sequences of the same length\n",
        "      5. NER label sequences are encoded with a one-hot scheme and weighted to overcome the imbalanced classesissue\n",
        "  - neural-network model: \n",
        "      1. word embedding vectors are computed according to the method chosen\n",
        "      2. a bi-LSTM neural network is instantiated and fitted with the training data\n",
        "  - model evaluation: the model is evaluated with an entity-level F1-score\n",
        "      1. we remove padding from sentences and convert data into their initial table format \n",
        "      2. we convert NER labels into the BIO2 scheme\n",
        "      3. we evaluate the fitted model on the development dataset\n",
        "\n",
        "\n",
        "All this work is detailed in my report.\n",
        "\n",
        "\n",
        "NB: in this notebook, the BIO tagging scheme refers to the BIO2 one, described in the report.\n"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SVzUdBszzWqF"
      },
      "source": [
        "## Import libraries"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "GezXZk_AnWFo"
      },
      "source": [
        "# import usefule libraries\n",
        "import pandas as pd\n",
        "import numpy as np\n",
        "import matplotlib.pyplot as plt\n",
        "from collections import Counter\n",
        "import copy\n",
        "from sklearn.utils.class_weight import compute_class_weight\n",
        "\n",
        "# load Keras and TensorFlow\n",
        "from tensorflow import keras\n",
        "import tensorflow as tf\n",
        "from keras.preprocessing.sequence import pad_sequences\n",
        "from keras.utils import to_categorical\n",
        "\n",
        "# word embedding models\n",
        "import gensim.downloader\n",
        "from gensim.test.utils import common_texts\n",
        "from gensim.models import Word2Vec, FastText\n",
        "from gensim import models"
      ],
      "execution_count": 1,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JCPbqLfXocAk"
      },
      "source": [
        "# Global variable"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "NfxvI_BJodnt"
      },
      "source": [
        "ner_scheme = 'IO'"
      ],
      "execution_count": 2,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "Y_j_Wi_RmXt4"
      },
      "source": [
        "# Data Pre-processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vhaxoj0YnSxi"
      },
      "source": [
        "Load data from the W-NUT 2017 Shared Task"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "x4qjV6n2tZkq",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "385b6565-9de4-41bf-e8f8-478fe33266a5"
      },
      "source": [
        "# load training data\n",
        "wnuttrain = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17train_clean_tagged.txt'\n",
        "train = pd.read_table(wnuttrain, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "train.head()"
      ],
      "execution_count": 3,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@paulwalk</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>It</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PRON</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'s</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>AUX</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>DET</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>view</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>NOUN</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "       token label bio_only  upos\n",
              "0  @paulwalk     O        O  NOUN\n",
              "1         It     O        O  PRON\n",
              "2         's     O        O   AUX\n",
              "3        the     O        O   DET\n",
              "4       view     O        O  NOUN"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 3
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "yPNWhcjo83aQ"
      },
      "source": [
        "### Tagging scheme conversion"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hJbWe4FHncQ-"
      },
      "source": [
        "All the following functions deal with the NER tags. They whether convert the current tagging scheme into another one, convert labels into integers or retrieve labels from integers."
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "vjJXeAqUB0Pi"
      },
      "source": [
        "#### BIO functions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "wQ6DPuF5o4tx"
      },
      "source": [
        "NB: in this notebook, the BIO tagging scheme refers to the BIO2 one described in the report."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Sg3uHpU8ubry"
      },
      "source": [
        "# training labels: convert BIO to integers\n",
        "def bio_index(bio):\n",
        "  ind = bio\n",
        "  if not pd.isnull(bio):  # deal with empty lines\n",
        "    if bio=='B':\n",
        "      ind = 0\n",
        "    elif bio=='I':\n",
        "      ind = 1\n",
        "    elif bio=='O':\n",
        "      ind = 2\n",
        "  return ind\n",
        "\n",
        "\n",
        "# function to convert BIO indices into BIO labels\n",
        "def reverse_bio(ind):\n",
        "  bio = 'B'\n",
        "  if ind==0:\n",
        "    bio = 'B'\n",
        "  elif ind==1:\n",
        "    bio = 'I'\n",
        "  elif ind==2:\n",
        "    bio = 'O'\n",
        "  return bio\n",
        "\n",
        "\n",
        "# function to rectify BIO predictions\n",
        "def correct_preds(preds):\n",
        "  for i in range(len(preds)):\n",
        "    if i == 0:\n",
        "      if preds[i] == 'I':\n",
        "        preds[i] = 'B'\n",
        "    \n",
        "    else:\n",
        "      if preds[i] == 'B' and preds[i-1] != 'O':\n",
        "        preds[i] = 'I'\n",
        "      elif preds[i] == 'I' and preds[i-1] == 'O':\n",
        "        preds[i] = 'B'\n",
        "  \n",
        "  return preds"
      ],
      "execution_count": 4,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mntF9f09fyGH"
      },
      "source": [
        "#### BIO to BIO1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "3kwr7zqjfyGN"
      },
      "source": [
        "# function to convert BIO labels into BIO1 ones\n",
        "def bio_to_bio1(bio):\n",
        "  bio1 = bio.copy()\n",
        "  for i in range(len(bio)):\n",
        "    if not pd.isnull(bio[i]):\n",
        "      if bio[i] == 'O':\n",
        "        bio1[i] = 'O'\n",
        "      elif bio[i] == 'I':\n",
        "        bio1[i] = 'I'\n",
        "      else:\n",
        "        if i != 0 and bio[i-1] == 'I':\n",
        "          bio1[i] = 'B'\n",
        "        else:\n",
        "          bio1[i] = 'I'\n",
        "  return bio1\n",
        "\n",
        "\n",
        "# function to convert BIO1 labels into BIO ones\n",
        "def bio1_to_bio(bio1):\n",
        "  bio = bio1.copy()\n",
        "  for i in range(len(bio1)):\n",
        "    if not pd.isnull(bio1[i]):\n",
        "      if bio1[i] == 'O':\n",
        "        bio[i] = 'O'\n",
        "      elif bio1[i] == 'B':\n",
        "        bio[i] = 'B'\n",
        "      else:\n",
        "        if i == 0 or (i != 0 and bio1[i-1] in ['O', None]):\n",
        "          bio[i] = 'B'\n",
        "        else:\n",
        "          bio[i] = 'I'\n",
        "  return bio\n",
        "\n",
        "\n",
        "# training labels: convert BIO1 to integers\n",
        "def bio1_index(bio1):\n",
        "  ind = bio1\n",
        "  if not pd.isnull(bio1):  # deal with empty lines\n",
        "    if bio1=='B':\n",
        "      ind = 0\n",
        "    elif bio1=='I':\n",
        "      ind = 1\n",
        "    elif bio1=='O':\n",
        "      ind = 2\n",
        "  return ind\n",
        "\n",
        "\n",
        "# function to convert BIO1 indices into BIO1 labels\n",
        "def reverse_bio1(ind):\n",
        "  bio1 = 'B'\n",
        "  if ind==0:\n",
        "    bio1 = 'B'\n",
        "  elif ind==1:\n",
        "    bio1 = 'I'\n",
        "  elif ind==2:\n",
        "    bio1 = 'O'\n",
        "  return bio1"
      ],
      "execution_count": 5,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MQRwbF8RBjbL"
      },
      "source": [
        "#### BIO to IOE1"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "pVv1NlwTBon_"
      },
      "source": [
        "# function to convert BIO labels into IOE1 ones\n",
        "def bio_to_ioe1(bio):\n",
        "  ioe1 = bio.copy()\n",
        "  for i in range(len(bio)):\n",
        "    if not pd.isnull(bio[i]):\n",
        "      if bio[i] == 'O':\n",
        "        ioe1[i] = 'O'\n",
        "      else:\n",
        "        if i != len(bio)-1 and bio[i+1] == 'B':\n",
        "          ioe1[i] = 'E'\n",
        "        else:\n",
        "          ioe1[i] = 'I'\n",
        "  return ioe1\n",
        "\n",
        "\n",
        "# function to convert IOE1 labels into BIO ones\n",
        "def ioe1_to_bio(ioe1):\n",
        "  bio = ioe1.copy()\n",
        "  for i in range(len(ioe1)):\n",
        "    if not pd.isnull(ioe1[i]):\n",
        "      if ioe1[i] == 'O':\n",
        "        bio[i] = 'O'\n",
        "      else:\n",
        "        if i == 0 or (i != 0 and ioe1[i-1] in ['O', 'E', None]):\n",
        "          bio[i] = 'B'\n",
        "        else:\n",
        "          bio[i] = 'I'\n",
        "  return bio\n",
        "\n",
        "\n",
        "# training labels: convert IOE1 to integers\n",
        "def ioe1_index(ioe1):\n",
        "  ind = ioe1\n",
        "  if not pd.isnull(ioe1):  # deal with empty lines\n",
        "    if ioe1=='I':\n",
        "      ind = 0\n",
        "    elif ioe1=='O':\n",
        "      ind = 1\n",
        "    elif ioe1=='E':\n",
        "      ind = 2\n",
        "  return ind\n",
        "\n",
        "\n",
        "# function to convert IOE1 indices into IOE1 labels\n",
        "def reverse_ioe1(ind):\n",
        "  ioe1 = 'I'\n",
        "  if ind==0:\n",
        "    ioe1 = 'I'\n",
        "  elif ind==1:\n",
        "    ioe1 = 'O'\n",
        "  elif ind==2:\n",
        "    ioe1 = 'E'\n",
        "  return ioe1"
      ],
      "execution_count": 6,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "hnwM4TAKcQkq"
      },
      "source": [
        "#### BIO to IOE2"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Wn5nlI5xcQky"
      },
      "source": [
        "# function to convert BIO labels into IOE2 ones\n",
        "def bio_to_ioe2(bio):\n",
        "  ioe2 = bio.copy()\n",
        "  for i in range(len(bio)):\n",
        "    if not pd.isnull(bio[i]):\n",
        "      if bio[i] == 'O':\n",
        "        ioe2[i] = 'O'\n",
        "      else:\n",
        "        if i == len(bio)-1 or bio[i+1] != 'I':\n",
        "          ioe2[i] = 'E'\n",
        "        else:\n",
        "          ioe2[i] = 'I'\n",
        "  return ioe2\n",
        "\n",
        "\n",
        "# function to convert IOE2 labels into BIO ones\n",
        "def ioe2_to_bio(ioe2):\n",
        "  bio = ioe2.copy()\n",
        "  for i in range(len(ioe2)):\n",
        "    if not pd.isnull(ioe2[i]):\n",
        "      if (ioe2[i] == 'O'):\n",
        "        bio[i] = 'O'\n",
        "      else:\n",
        "        if i == 0 or (i != 0 and ioe2[i-1] in ['O', 'E', None]):\n",
        "          bio[i] = 'B'\n",
        "        else:\n",
        "          bio[i] = 'I'\n",
        "  return bio\n",
        "\n",
        "\n",
        "# training labels: convert IOE2 to integers\n",
        "def ioe2_index(ioe2):\n",
        "  ind = ioe2\n",
        "  if not pd.isnull(ioe2):  # deal with empty lines\n",
        "    if ioe2=='I':\n",
        "      ind = 0\n",
        "    elif ioe2=='O':\n",
        "      ind = 1\n",
        "    elif ioe2=='E':\n",
        "      ind = 2\n",
        "  return ind\n",
        "\n",
        "\n",
        "# function to convert IOE2 indices into IOE2 labels\n",
        "def reverse_ioe2(ind):\n",
        "  ioe2 = 'I'\n",
        "  if ind==0:\n",
        "    ioe2 = 'I'\n",
        "  elif ind==1:\n",
        "    ioe2 = 'O'\n",
        "  elif ind==2:\n",
        "    ioe2 = 'E'\n",
        "  return ioe2"
      ],
      "execution_count": 7,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "QZ-8aXT4Bf6r"
      },
      "source": [
        "#### BIO to IO"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "6RsXbOSyBQMC"
      },
      "source": [
        "# function to convert BIO labels into IO ones\n",
        "def bio_to_io(bio):\n",
        "  io = bio.copy()\n",
        "  for i in range(len(bio)):\n",
        "    if not pd.isnull(bio[i]):\n",
        "      if bio[i] == 'O':\n",
        "        io[i] = 'O'\n",
        "      else:\n",
        "        io[i] = 'I'\n",
        "  return io\n",
        "\n",
        "\n",
        "# function to convert IO labels into BIO ones\n",
        "def io_to_bio(io):\n",
        "  bio = io.copy()\n",
        "  for i in range(len(io)):\n",
        "    if not pd.isnull(io[i]):\n",
        "      if (io[i] == 'O'):\n",
        "        bio[i] = 'O'\n",
        "      else:\n",
        "        if i == 0 or (i != 0 and io[i-1] != 'I'):\n",
        "          bio[i] = 'B'\n",
        "        else:\n",
        "          bio[i] = 'I'\n",
        "  return bio\n",
        "\n",
        "\n",
        "# training labels: convert BIO to integers\n",
        "def io_index(io):\n",
        "  ind = io\n",
        "  if not pd.isnull(io):  # deal with empty lines\n",
        "    if io=='I':\n",
        "      ind = 0\n",
        "    elif io=='O':\n",
        "      ind = 1\n",
        "  return ind\n",
        "\n",
        "\n",
        "# function to convert IO indices into IO labels\n",
        "def reverse_io(ind):\n",
        "  io = 'I'\n",
        "  if ind==0:\n",
        "    io = 'I'\n",
        "  elif ind==1:\n",
        "    io = 'O'\n",
        "  return io"
      ],
      "execution_count": 8,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "d6H0xHZ3BtVL"
      },
      "source": [
        "#### BIO to BILOU"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Z9N5XgoCBy1U"
      },
      "source": [
        "# function to convert BIO labels into BILOU ones\n",
        "def bio_to_bilou(bio):\n",
        "  bilou = bio.copy()\n",
        "  for i in range(len(bio)):\n",
        "    if not pd.isnull(bio[i]):\n",
        "      if i == len(bio)-1 :\n",
        "        if bio[i] == 'B':\n",
        "          bilou[i] = 'U'\n",
        "        elif bio[i] == 'I':\n",
        "          bilou[i] = 'L'\n",
        "        else:\n",
        "          bilou[i] = 'O'\n",
        "      \n",
        "      else:\n",
        "        if bio[i] == 'O':\n",
        "          bilou[i] = 'O'\n",
        "        elif bio[i] == 'B':\n",
        "          if bio[i+1] == 'I':\n",
        "            bilou[i] = 'B'\n",
        "          else:\n",
        "            bilou[i] = 'U'\n",
        "        else:\n",
        "          if bio[i+1] == 'I':\n",
        "            bilou[i] = 'I'\n",
        "          else:\n",
        "            bilou[i] = 'L'\n",
        "    \n",
        "  return bilou\n",
        "\n",
        "\n",
        "# function to convert BILOU labels into BIO ones\n",
        "def bilou_to_bio(bilou):\n",
        "  bio = bilou.copy()\n",
        "  for i in range(len(bilou)):\n",
        "    if not pd.isnull(bilou[i]):\n",
        "      if (bilou[i] == 'B') or (bilou[i] == 'U'):\n",
        "        bio[i] = 'B'\n",
        "      elif (bilou[i] == 'I') or (bilou[i] == 'L'):\n",
        "        bio[i] = 'I'\n",
        "      else:\n",
        "        bio[i] = 'O'\n",
        "  return bio\n",
        "\n",
        "\n",
        "# training labels: convert BIO to integers\n",
        "def bilou_index(bilou):\n",
        "  ind = bilou\n",
        "  if not pd.isnull(bilou):  # deal with empty lines\n",
        "    if bilou=='B':\n",
        "      ind = 0\n",
        "    elif bilou=='I':\n",
        "      ind = 1\n",
        "    elif bilou=='L':\n",
        "      ind = 2\n",
        "    elif bilou=='O':\n",
        "      ind = 3\n",
        "    elif bilou=='U':\n",
        "      ind = 4\n",
        "  return ind\n",
        "\n",
        "\n",
        "# function to convert BILOU indices into BILOU labels\n",
        "def reverse_bilou(ind):\n",
        "  bilou = 'B'\n",
        "  if ind==0:\n",
        "    bilou = 'B'\n",
        "  elif ind==1:\n",
        "    bilou = 'I'\n",
        "  elif ind==2:\n",
        "    bilou = 'L'\n",
        "  elif ind==3:\n",
        "    bilou = 'O'\n",
        "  elif ind==4:\n",
        "    bilou = 'U'\n",
        "  return bilou"
      ],
      "execution_count": 9,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "zaggJ5jHn8oq"
      },
      "source": [
        "### Features extraction"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "SbwhprrloGN3"
      },
      "source": [
        "The following cell converts word tokens and NER labels into integers, and transform tokens to lower case."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UwMBhBFZrm3Z",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 947
        },
        "outputId": "1c95508e-9102-4464-854d-1bdc1052ab92"
      },
      "source": [
        "# in order to convert word tokens to integers: list the set of token types\n",
        "token_vocab = train.token.unique().tolist()\n",
        "oov = len(token_vocab)  # OOV (out of vocabulary) token as vocab length (because that's max.index + 1)\n",
        "\n",
        "# convert word tokens to integers\n",
        "def token_index(tok):\n",
        "  ind = tok\n",
        "  if not pd.isnull(tok):  # new since last time: deal with the empty lines which we didn't drop yet\n",
        "    if tok in token_vocab:  # if token in vocabulary\n",
        "      ind = token_vocab.index(tok)\n",
        "    else:  # else it's OOV\n",
        "      ind = oov\n",
        "  return ind\n",
        "\n",
        "\n",
        "# convert word tokens to lower case\n",
        "def token_lower(tok):\n",
        "  low = tok\n",
        "  if not pd.isnull(tok):  # new since last time: deal with the empty lines which we didn't drop yet\n",
        "    low = tok.lower()\n",
        "  return low\n",
        "\n",
        "\n",
        "# pass a data frame through our feature extractor\n",
        "def extract_features(txt_orig, ner_scheme=ner_scheme, istest=False):\n",
        "  txt = txt_orig.copy()\n",
        "  tokinds = [token_index(u) for u in txt['token']]\n",
        "  txt['token_indices'] = tokinds\n",
        "  toklows = [token_lower(u) for u in txt['token']]\n",
        "  txt['token'] = toklows\n",
        "  if not istest:  # can't do this with the test set\n",
        "    if (ner_scheme == 'IO'):\n",
        "      txt['bio_only'] = bio_to_io(txt['bio_only'])\n",
        "      bioints = [io_index(b) for b in txt['bio_only']]\n",
        "    elif (ner_scheme == 'BILOU'):\n",
        "      txt['bio_only'] = bio_to_bilou(txt['bio_only'])\n",
        "      bioints = [bilou_index(b) for b in txt['bio_only']]\n",
        "    elif (ner_scheme == 'IOE1'):\n",
        "      txt['bio_only'] = bio_to_ioe1(txt['bio_only'])\n",
        "      bioints = [ioe1_index(b) for b in txt['bio_only']]\n",
        "    elif (ner_scheme == 'IOE2'):\n",
        "      txt['bio_only'] = bio_to_ioe2(txt['bio_only'])\n",
        "      bioints = [ioe2_index(b) for b in txt['bio_only']]\n",
        "    elif (ner_scheme == 'BIO1'):\n",
        "      txt['bio_only'] = bio_to_bio1(txt['bio_only'])\n",
        "      bioints = [bio1_index(b) for b in txt['bio_only']]\n",
        "    elif (ner_scheme == 'BIO'):\n",
        "      bioints = [bio_index(b) for b in txt['bio_only']]\n",
        "    txt['bio_only'] = bioints\n",
        "  return txt\n",
        "\n",
        "train_copy = extract_features(train)\n",
        "train_copy.head(n=30)"
      ],
      "execution_count": 10,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>label</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "      <th>token_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>@paulwalk</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>0.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>it</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PRON</td>\n",
              "      <td>1.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>'s</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>AUX</td>\n",
              "      <td>2.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>the</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>DET</td>\n",
              "      <td>3.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>view</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>4.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>5</th>\n",
              "      <td>from</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>5.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>6</th>\n",
              "      <td>where</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>6.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>7</th>\n",
              "      <td>i</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PRON</td>\n",
              "      <td>7.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>8</th>\n",
              "      <td>'m</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>X</td>\n",
              "      <td>8.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>9</th>\n",
              "      <td>living</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>9.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>10</th>\n",
              "      <td>for</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>10.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>11</th>\n",
              "      <td>two</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NUM</td>\n",
              "      <td>11.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>12</th>\n",
              "      <td>weeks</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>12.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>13</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>14</th>\n",
              "      <td>empire</td>\n",
              "      <td>B-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>14.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>15</th>\n",
              "      <td>state</td>\n",
              "      <td>I-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>15.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>16</th>\n",
              "      <td>building</td>\n",
              "      <td>I-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>16.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>17</th>\n",
              "      <td>=</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>SYM</td>\n",
              "      <td>17.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>18</th>\n",
              "      <td>esb</td>\n",
              "      <td>B-location</td>\n",
              "      <td>0.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>18.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>19</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>20</th>\n",
              "      <td>pretty</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>19.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>21</th>\n",
              "      <td>bad</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>20.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>22</th>\n",
              "      <td>storm</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>21.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>23</th>\n",
              "      <td>here</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADV</td>\n",
              "      <td>22.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>24</th>\n",
              "      <td>last</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADJ</td>\n",
              "      <td>23.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>25</th>\n",
              "      <td>evening</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>NOUN</td>\n",
              "      <td>24.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>26</th>\n",
              "      <td>.</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>13.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>27</th>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "      <td>NaN</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>28</th>\n",
              "      <td>from</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>ADP</td>\n",
              "      <td>26.0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>29</th>\n",
              "      <td>green</td>\n",
              "      <td>O</td>\n",
              "      <td>1.0</td>\n",
              "      <td>PROPN</td>\n",
              "      <td>27.0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "        token       label  bio_only   upos  token_indices\n",
              "0   @paulwalk           O       1.0   NOUN            0.0\n",
              "1          it           O       1.0   PRON            1.0\n",
              "2          's           O       1.0    AUX            2.0\n",
              "3         the           O       1.0    DET            3.0\n",
              "4        view           O       1.0   NOUN            4.0\n",
              "5        from           O       1.0    ADP            5.0\n",
              "6       where           O       1.0    ADV            6.0\n",
              "7           i           O       1.0   PRON            7.0\n",
              "8          'm           O       1.0      X            8.0\n",
              "9      living           O       1.0   NOUN            9.0\n",
              "10        for           O       1.0    ADP           10.0\n",
              "11        two           O       1.0    NUM           11.0\n",
              "12      weeks           O       1.0   NOUN           12.0\n",
              "13          .           O       1.0  PUNCT           13.0\n",
              "14     empire  B-location       0.0  PROPN           14.0\n",
              "15      state  I-location       0.0  PROPN           15.0\n",
              "16   building  I-location       0.0  PROPN           16.0\n",
              "17          =           O       1.0    SYM           17.0\n",
              "18        esb  B-location       0.0  PROPN           18.0\n",
              "19          .           O       1.0  PUNCT           13.0\n",
              "20     pretty           O       1.0    ADV           19.0\n",
              "21        bad           O       1.0    ADJ           20.0\n",
              "22      storm           O       1.0   NOUN           21.0\n",
              "23       here           O       1.0    ADV           22.0\n",
              "24       last           O       1.0    ADJ           23.0\n",
              "25    evening           O       1.0   NOUN           24.0\n",
              "26          .           O       1.0  PUNCT           13.0\n",
              "27        NaN         NaN       NaN    NaN            NaN\n",
              "28       from           O       1.0    ADP           26.0\n",
              "29      green           O       1.0  PROPN           27.0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 10
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "mgGAj3CypNia"
      },
      "source": [
        "### Data formatting"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "MF3OhgAApPcV"
      },
      "source": [
        "The following functions convert table-format data into sequences and pad sequences to make them of the same length. To do so, we find the longuest sequence in all datasets, and add a padding element to sequences to make them of the length of the longuest sequence."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "kF6c9JrvYTa3",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 298
        },
        "outputId": "d0ea706a-0e6d-42bf-f4f7-9110b576ac71"
      },
      "source": [
        "def tokens2sequences(txt_orig,istest=False):\n",
        "  '''\n",
        "  Takes panda dataframe as input, copies, and adds a sequence index based on full-stops.\n",
        "  Outputs a dataframe with sequences of tokens, named entity labels, and token indices as lists.\n",
        "  '''\n",
        "  txt = txt_orig.copy()\n",
        "  txt['sequence_num'] = 0\n",
        "  seqcount = 0\n",
        "  for i in txt.index:  # in each row...\n",
        "    txt.loc[i,'sequence_num'] = seqcount  # set the sequence number\n",
        "    if pd.isnull(txt.loc[i,'token']):  # increment sequence counter at empty lines\n",
        "      seqcount += 1\n",
        "  # now drop the empty lines, group by sequence number and output df of sequence lists\n",
        "  txt = txt.dropna()\n",
        "  if istest:  # test set doesn't have labels\n",
        "    txt_seqs = txt.groupby(['sequence_num'],as_index=False)[['token', 'token_indices']].agg(lambda x: list(x))\n",
        "  else:\n",
        "    txt_seqs = txt.groupby(['sequence_num'],as_index=False)[['token', 'bio_only', 'token_indices']].agg(lambda x: list(x))\n",
        "  return txt_seqs\n",
        "\n",
        "print(\"This cell takes a little while to run: be patient :)\")\n",
        "train_seqs = tokens2sequences(train_copy)\n",
        "train_seqs.head()"
      ],
      "execution_count": 11,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "This cell takes a little while to run: be patient :)\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[@paulwalk, it, 's, the, view, from, where, i,...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[from, green, newsfeed, :, ahfa, extends, dead...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[pxleyes, top, 50, photography, contest, pictu...</td>\n",
              "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[today, is, my, last, day, at, the, office, .]</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0]</td>\n",
              "      <td>[51.0, 52.0, 53.0, 23.0, 54.0, 55.0, 3.0, 56.0...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[4dbling, 's, place, til, monday, ,, party, pa...</td>\n",
              "      <td>[0.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                      token_indices\n",
              "0             0  ...  [0.0, 1.0, 2.0, 3.0, 4.0, 5.0, 6.0, 7.0, 8.0, ...\n",
              "1             1  ...  [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
              "2             2  ...  [39.0, 40.0, 41.0, 42.0, 43.0, 44.0, 45.0, 46....\n",
              "3             3  ...  [51.0, 52.0, 53.0, 23.0, 54.0, 55.0, 3.0, 56.0...\n",
              "4             4  ...  [57.0, 2.0, 58.0, 59.0, 60.0, 61.0, 62.0, 62.0...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 11
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "263V10sYg2l-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e19458f5-e1be-4223-d2f3-fbac2623f760"
      },
      "source": [
        "def find_longest_sequence(txt,longest_seq):\n",
        "  '''find the longest sequence in the dataframe'''\n",
        "  seq = \"\"\n",
        "  for i in txt.index:\n",
        "    seqlen = len(txt['token'][i])\n",
        "    if seqlen > longest_seq:  # update high water mark if new longest sequence encountered\n",
        "      longest_seq = seqlen\n",
        "      seq = txt['token'][i]\n",
        "  return longest_seq, seq\n",
        "\n",
        "train_longest, train_seq = find_longest_sequence(train_seqs,0)\n",
        "print('The longest sequence in the training set is %i tokens long:' % train_longest)\n",
        "print(train_seq)"
      ],
      "execution_count": 12,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The longest sequence in the training set is 41 tokens long:\n",
            "['re', ':', 're', ':', 're', ':', 're', ':', 're', ':', 're', ':', 're', ':', 're', ':', 're', ':', 're', ':', 're', ':', 're', ':', 'esther', 'sikkimese', 'is', 'now', 'following', 'me', 'on', 'twitter', '!', 'http://t.co/z58brwgxfp', 'thanks', 'a', 'bunch', '!', '103', 'january', '...']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "sQzXQaJpkyfr",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "31d49387-7859-4ada-c302-74b6ca2a5878"
      },
      "source": [
        "# the dev set\n",
        "wnutdev = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17dev_clean_tagged.txt'\n",
        "dev = pd.read_table(wnutdev, header=None, names=['token', 'label', 'bio_only', 'upos'])\n",
        "dev_copy = extract_features(dev)\n",
        "dev_seqs = tokens2sequences(dev_copy)\n",
        "dev_longest, dev_seq = find_longest_sequence(dev_seqs,0)\n",
        "print('The longest sequence in the dev set is %i tokens long:' % dev_longest)\n",
        "print(dev_seq)\n",
        "\n",
        "# the test set\n",
        "wnuttest = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_clean_tagged.txt'\n",
        "test = pd.read_table(wnuttest, header=None, names=['token', 'upos'])\n",
        "test_copy = extract_features(test, istest=True)\n",
        "test_seqs = tokens2sequences(test_copy, True)\n",
        "test_longest, test_seq = find_longest_sequence(test_seqs,0)\n",
        "print('The longest sequence in the test set is %i tokens long:' % test_longest)\n",
        "print(test_seq)"
      ],
      "execution_count": 13,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The longest sequence in the dev set is 82 tokens long:\n",
            "['link', 'should', 'not', 'hold', 'knives', 'or', 'dogs', '.', 'what', \"'\", 's', 'with', 'all', 'that', 'excessive', 'rubbing', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '?', '/', '/', '/']\n",
            "The longest sequence in the test set is 105 tokens long:\n",
            "['in', 'order', 'to', 'calculate', 'anything', ',', 'more', 'input', 'data', 'is', 'required', ';', 'such', 'as', ':', '\"\"\"\"', 'from', 'where', 'do', 'you', 'hit', 'the', 'ball', 'and', 'at', 'what', 'angle', '?', 'do', 'you', 'want', 'to', 'take', 'into', 'account', 'effects', 'due', 'to', 'the', 'spin', 'of', 'the', 'ball', '?', 'should', 'friction', 'be', 'included', '?', '\"\"\"\"', 'generally', '(', 'if', 'you', 'do', 'n', \"'\", 't', 'put', 'any', 'restrictions', 'on', 'how', 'you', 'can', 'hit', 'it', ')', 'there', 'is', 'no', 'reason', 'that', 'you', 'could', 'not', 'make', 'the', 'ball', 'return', '(', 'unless', 'you', \"'\", 'd', 'have', 'to', 'hit', 'it', 'so', 'hard', 'that', 'the', 'ball', 'breaks', '(', 'which', 'i', 'doubt', 'would', 'be', 'the', 'case', ')', '.']\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "U3Spc-hutkTG",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "391db860-3fa4-4347-9c77-4300506f9c4a"
      },
      "source": [
        "# set maximum sequence length\n",
        "seq_length = max(train_longest, dev_longest, test_longest)\n",
        "\n",
        "# a new dummy token index, one more than OOV\n",
        "padtok = oov+1\n",
        "print('The padding token index is %i' % padtok)\n",
        "\n",
        "# use pad_sequences, padding or truncating at the end of the sequence (default is 'pre')\n",
        "train_seqs_padded = pad_sequences(train_seqs['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                  dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "print('Example of padded token sequence:')\n",
        "print(train_seqs_padded[1])"
      ],
      "execution_count": 14,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "The padding token index is 14802\n",
            "Example of padded token sequence:\n",
            "[   26    27    28    29    30    31    32    10    33    34    35    36\n",
            "    13    37    38 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802 14802\n",
            " 14802 14802 14802 14802 14802 14802 14802 14802 14802]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ITjJLUqmpjvP"
      },
      "source": [
        "NER labels are converted into a one-hot scheme."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0iRzVlQ8MKxE",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "06fc0bf5-c2a3-4a80-c8fc-10ab10e11666"
      },
      "source": [
        "# get lists of named entity labels, padded with a null label\n",
        "if ner_scheme in ['BIO', 'IOE2', 'IOE1','BIO1']:\n",
        "  padlab = 3\n",
        "elif ner_scheme == 'IO':\n",
        "  padlab = 2\n",
        "elif ner_scheme == 'BILOU':\n",
        "  padlab = 5\n",
        "\n",
        "train_labs_padded = pad_sequences(train_seqs['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                  dtype='int32', padding='post', truncating='post', value=padlab)\n",
        "\n",
        "# convert those labels to one-hot encoding\n",
        "n_labs = padlab + 1  # we have 2, 3 or 4 labels: B, I, O (0, 1, 2) + the pad label 3\n",
        "train_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in train_labs_padded]\n",
        "\n",
        "# follow the print outputs below to see how the labels are transformed\n",
        "print('Example of padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(train_seqs.loc[1])\n",
        "print('Length of input sequence: %i' % len(train_labs_padded[1]))\n",
        "print('Length of label sequence: %i' % len(train_labs_onehot[1]))\n",
        "print(train_labs_padded[1][:11])\n",
        "print(train_labs_onehot[1][:11])"
      ],
      "execution_count": 15,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Example of padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     1\n",
            "token            [from, green, newsfeed, :, ahfa, extends, dead...\n",
            "bio_only         [1.0, 1.0, 1.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, ...\n",
            "token_indices    [26.0, 27.0, 28.0, 29.0, 30.0, 31.0, 32.0, 10....\n",
            "Name: 1, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of label sequence: 105\n",
            "[1 1 1 1 0 1 1 1 1 1 1]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "BBy8RT93PG6A",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "38cf60c1-8654-4569-988e-4a351c7e4dd3"
      },
      "source": [
        "# now process the dev set in the same way: padding the tokens & labels, and one-hot encoding the labels\n",
        "dev_seqs_padded = pad_sequences(dev_seqs['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "dev_labs_padded = pad_sequences(dev_seqs['bio_only'].tolist(), maxlen=seq_length,\n",
        "                                dtype='int32', padding='post', truncating='post', value=padlab)\n",
        "dev_labs_onehot = [to_categorical(i, num_classes=n_labs) for i in dev_labs_padded]\n",
        "\n",
        "print('Dev set padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(dev_seqs.loc[2])\n",
        "print('Length of input sequence: %i' % len(dev_labs_padded[1]))\n",
        "print('Length of label sequence: %i' % len(dev_labs_onehot[1]))\n",
        "print(dev_labs_padded[2][:11])\n",
        "print(dev_labs_onehot[2][:11])"
      ],
      "execution_count": 16,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dev set padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     2\n",
            "token            [all, i, ', ve, been, doing, is, binge, watchi...\n",
            "bio_only         [1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...\n",
            "token_indices    [405.0, 7.0, 573.0, 12927.0, 90.0, 848.0, 52.0...\n",
            "Name: 2, dtype: object\n",
            "Length of input sequence: 105\n",
            "Length of label sequence: 105\n",
            "[1 1 1 1 1 1 1 1 1 0 0]\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [1. 0. 0.]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "ttreFiVbp10T"
      },
      "source": [
        "NER labels are weighted to overcome the imbalanced classes issue: weights are inversely proportional to classes occurrences in order that common classes have a lower weight."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "ZzFn-ZoBtUUK",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "1b6bfd7c-1cd6-4bc8-c859-ecdf333972af"
      },
      "source": [
        "# use deep copy to ensure we aren't updating original values\n",
        "train_weights_onehot = copy.deepcopy(train_labs_onehot)\n",
        "\n",
        "if ner_scheme == 'BIO':\n",
        "  y_integers = [0]*1964 + [1]*1177 + [2]*59095 + [3]*292139\n",
        "if ner_scheme == 'BIO1':\n",
        "  y_integers = [0]*13 + [1]*3128 + [2]*59095 + [3]*292139\n",
        "elif ner_scheme == 'IOE1':\n",
        "  y_integers = [0]*3115 + [1]*59095 + [2]*26 + [3]*292139 \n",
        "elif ner_scheme == 'IOE2':\n",
        "  y_integers = [0]*1177 + [1]*59095 + [2]*1964 + [3]*292139 \n",
        "if ner_scheme == 'IO':\n",
        "  y_integers = [0]*3141 + [1]*59095 + [2]*292139  \n",
        "elif ner_scheme == 'BILOU':\n",
        "  y_integers = [0]*786 + [1]*391 + [2]*786 + [3]*59095 + [4]*1178 + [5]*292139  \n",
        "\n",
        "class_weights = compute_class_weight('balanced', np.unique(y_integers), y_integers)\n",
        "class_wts = list(dict(enumerate(class_weights)).values())\n",
        "\n",
        "\n",
        "# apply our weights to the label lists\n",
        "for i,labs in enumerate(train_weights_onehot):\n",
        "  for j,lablist in enumerate(labs):\n",
        "    lablistaslist = lablist.tolist()\n",
        "    whichismax = lablistaslist.index(max(lablistaslist))\n",
        "    train_weights_onehot[i][j][whichismax] = class_wts[whichismax]\n",
        "\n",
        "# what's this like, before and after?\n",
        "print('Initial one-hot label encoding:')\n",
        "print(train_labs_onehot[1][:11])\n",
        "\n",
        "print('Weighted label encoding:')\n",
        "print(train_weights_onehot[1][:11])"
      ],
      "execution_count": 17,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Initial one-hot label encoding:\n",
            "[[0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [1. 0. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]\n",
            " [0. 1. 0.]]\n",
            "Weighted label encoding:\n",
            "[[ 0.       1.9989   0.     ]\n",
            " [ 0.       1.9989   0.     ]\n",
            " [ 0.       1.9989   0.     ]\n",
            " [ 0.       1.9989   0.     ]\n",
            " [37.60745  0.       0.     ]\n",
            " [ 0.       1.9989   0.     ]\n",
            " [ 0.       1.9989   0.     ]\n",
            " [ 0.       1.9989   0.     ]\n",
            " [ 0.       1.9989   0.     ]\n",
            " [ 0.       1.9989   0.     ]\n",
            " [ 0.       1.9989   0.     ]]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YK8bwk-CmbF4"
      },
      "source": [
        "# Neural network model"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "GCtQr0X8p_O-"
      },
      "source": [
        "### Word embedding representation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "nckwb4ZEqDQp"
      },
      "source": [
        "When we work with pre-trained word representations, we first load or train these embedding methods with ```gensim``` and create an embedding matrix for our vocabulary list.\n",
        "\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Cu_QlkIDYzYR"
      },
      "source": [
        "embedding_dim = 300\n",
        "\n",
        "#word2vec = Word2Vec(sentences=list(train_seqs.token), size=embedding_dim, window=5, min_count=1, workers=4)\n",
        "#word_emb = word2vec.wv\n",
        "\n",
        "fasttext = FastText(sentences=list(train_seqs.token), size=embedding_dim, window=5, min_count=1, workers=4)\n",
        "word_emb = fasttext.wv\n",
        "\n",
        "#word_emb = gensim.downloader.load('word2vec-ruscorpora-300')\n",
        "#word_emb = gensim.downloader.load('fasttext-wiki-news-subwords-300')\n",
        "#word_emb = gensim.downloader.load('glove-twitter-25')\n",
        "#word_emb = gensim.downloader.load('glove-twitter-200')\n",
        "\n",
        "embedding_matrix = np.zeros((len(token_vocab)+2, embedding_dim))\n",
        "for i in range(len(token_vocab)):\n",
        "  try:\n",
        "    embedding_vector = fasttext_vectors[str(token_vocab[i]).lower()]\n",
        "    if embedding_vector is not None:\n",
        "      # words not found in embedding index will be all-zeros.\n",
        "      embedding_matrix[i] = embedding_vector\n",
        "  except:\n",
        "    continue"
      ],
      "execution_count": 18,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "3B7LWPpaGIqN"
      },
      "source": [
        "### Neural Network Classifier"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "9XoKcTQYq84E"
      },
      "source": [
        "Definition of our neural network classifier:\n",
        "\n",
        "We first prepare the input data as numpy arrays, list the metrics for model evaluation, define some important hyperparameters, then build the model layer by layer. It's a sequential model with an embedding layer followed by bidirectional LSTM, then a dropout layer before a final dense layer with softmax activation. We have early stopping criteria to halt training if improvements are not seen after 10 epochs."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Qi8Xpmfd75-C",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "c3d3db03-72a5-4fe0-8f11-da0dae0e3085"
      },
      "source": [
        "# prepare sequences and labels as numpy arrays, check dimensions\n",
        "X = np.array(train_seqs_padded)\n",
        "y = np.array(train_weights_onehot)\n",
        "print('Input sequence dimensions (n.docs, seq.length):')\n",
        "print(X.shape)\n",
        "print('Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):')\n",
        "print(y.shape)\n",
        "\n",
        "# our final vocab size is the padding token + 1 (OR length of vocab + OOV + PAD)\n",
        "vocab_size = padtok+1\n",
        "print(vocab_size==len(token_vocab)+2)\n",
        "embed_size = 128  # try an embedding size of 128 (could tune this)\n",
        "\n",
        "# list of metrics to use: true & false positives, negatives, accuracy, precision, recall, area under the curve\n",
        "METRICS = [\n",
        "      keras.metrics.TruePositives(name='tp'),\n",
        "      keras.metrics.FalsePositives(name='fp'),\n",
        "      keras.metrics.TrueNegatives(name='tn'),\n",
        "      keras.metrics.FalseNegatives(name='fn'), \n",
        "      keras.metrics.BinaryAccuracy(name='accuracy'),\n",
        "      keras.metrics.Precision(name='precision'),\n",
        "      keras.metrics.Recall(name='recall'),\n",
        "      keras.metrics.AUC(name='auc'),\n",
        "]\n",
        "\n",
        "# our model has the option for an label prediction bias, it's sequential, starts with an embedding layer, then bi-LSTM,\n",
        "# a dropout layer follows for regularisation, and a dense final layer with softmax activation to output class probabilities\n",
        "# we compile with the Adam optimizer at a low learning rate, use categorical cross-entropy as our loss function\n",
        "def make_model(metrics = METRICS, output_bias=None):\n",
        "  if output_bias is not None:\n",
        "    output_bias = tf.keras.initializers.Constant(output_bias)\n",
        "\n",
        "  if word_emb == None:\n",
        "      model = keras.Sequential([\n",
        "        keras.layers.Embedding(input_dim=vocab_size, output_dim=embed_size, input_length=seq_length, mask_zero=True, trainable=True),\n",
        "        keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2, recurrent_dropout=0)),  # 2 directions, 50 units each, concatenated (can change this)\n",
        "        keras.layers.Dropout(0.5),\n",
        "        keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias)),\n",
        "      ])\n",
        "  else:  # if we use pre-trained word representations, we provide the embedding layer with our embedding matrix created previously\n",
        "    model = keras.Sequential([\n",
        "      keras.layers.Embedding(input_dim=vocab_size, output_dim=embedding_dim, weights=[embedding_matrix], input_length=seq_length, trainable=True),\n",
        "      keras.layers.Bidirectional(keras.layers.LSTM(units=50, return_sequences=True, dropout=0.2, recurrent_dropout=0)),  # 2 directions, 50 units each, concatenated (can change this)\n",
        "      keras.layers.Dropout(0.5),\n",
        "      keras.layers.TimeDistributed(keras.layers.Dense(n_labs, activation='softmax', bias_initializer=output_bias)),\n",
        "    ])\n",
        "  model.compile(optimizer=keras.optimizers.Adam(lr=1e-3), loss=keras.losses.CategoricalCrossentropy(), metrics=metrics)\n",
        "  return model\n",
        "\n",
        "# early stopping criteria based on area under the curve: will stop if no improvement after 10 epochs\n",
        "early_stopping = tf.keras.callbacks.EarlyStopping(\n",
        "    monitor='val_auc', verbose=1, patience=10, mode='max', restore_best_weights=True)\n",
        "\n",
        "# the number of training epochs we'll use, and the batch size (how many texts are input at once)\n",
        "EPOCHS = 100\n",
        "BATCH_SIZE = 32\n",
        "\n",
        "print('\\n**Defining a neural network**')\n",
        "model = make_model()\n",
        "model.summary()"
      ],
      "execution_count": 19,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Input sequence dimensions (n.docs, seq.length):\n",
            "(3375, 105)\n",
            "Label dimensions (n.docs, seq.length, one-hot encoding of 4 NER labels):\n",
            "(3375, 105, 3)\n",
            "True\n",
            "\n",
            "**Defining a neural network**\n",
            "Model: \"sequential\"\n",
            "_________________________________________________________________\n",
            "Layer (type)                 Output Shape              Param #   \n",
            "=================================================================\n",
            "embedding (Embedding)        (None, 105, 300)          4440900   \n",
            "_________________________________________________________________\n",
            "bidirectional (Bidirectional (None, 105, 100)          140400    \n",
            "_________________________________________________________________\n",
            "dropout (Dropout)            (None, 105, 100)          0         \n",
            "_________________________________________________________________\n",
            "time_distributed (TimeDistri (None, 105, 3)            303       \n",
            "=================================================================\n",
            "Total params: 4,581,603\n",
            "Trainable params: 4,581,603\n",
            "Non-trainable params: 0\n",
            "_________________________________________________________________\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "cGIzukGWrac8"
      },
      "source": [
        "Because our dataset is highly imbalanced, we try to help the situation by setting an initial bias based on the label distribution in our dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "xEfwpTXVBLOC",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "ca8776f0-75a4-4c82-a50e-5cf1775898b6"
      },
      "source": [
        "# figure out the label distribution in our fixed-length texts\n",
        "all_labs = [l for lab in train_labs_padded for l in lab]\n",
        "label_count = Counter(all_labs)\n",
        "total_labs = len(all_labs)\n",
        "print(label_count)\n",
        "print(total_labs)\n",
        "\n",
        "# use this to define an initial model bias\n",
        "initial_bias=[]\n",
        "for i in range(len(label_count)):\n",
        "  initial_bias.append(label_count[i]/total_labs)\n",
        "print('Initial bias:')\n",
        "print(initial_bias)\n",
        "\n",
        "# pass the bias to the model and re-evaluate\n",
        "model = make_model(output_bias=initial_bias)\n",
        "results = model.evaluate(X, y, batch_size=BATCH_SIZE, verbose=0)\n",
        "print(\"Loss: {:0.4f}\".format(results[0]))"
      ],
      "execution_count": 20,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({2: 292139, 1: 59095, 0: 3141})\n",
            "354375\n",
            "Initial bias:\n",
            "[0.008863492063492063, 0.1667583774250441, 0.8243781305114638]\n",
            "Loss: 1.1642\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "CKzu8pUHsx4X"
      },
      "source": [
        "We fit our model with our training set and use the development set to evaluate metrics during training."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "H1GdpAQT9LOw",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "fd764c97-00bc-4311-a8c9-f5f28633a595"
      },
      "source": [
        "# prepare the dev sequences and labels as numpy arrays\n",
        "dev_X = np.array(dev_seqs_padded)\n",
        "dev_y = np.array(dev_labs_onehot)\n",
        "\n",
        "# re-initiate model with bias\n",
        "model = make_model(output_bias=initial_bias)\n",
        "\n",
        "# and fit...\n",
        "model.fit(X, y, batch_size=BATCH_SIZE, epochs=EPOCHS, callbacks = [early_stopping], validation_data=(dev_X, dev_y))"
      ],
      "execution_count": 39,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Epoch 1/100\n",
            "106/106 [==============================] - 29s 271ms/step - loss: 0.5069 - tp: 424769.0000 - fp: 17559.0000 - tn: 899721.0000 - fn: 33871.0000 - accuracy: 0.7286 - precision: 0.9603 - recall: 0.9261 - auc: 0.9939 - val_loss: 0.0742 - val_tp: 100216.0000 - val_fp: 3111.0000 - val_tn: 205419.0000 - val_fn: 4049.0000 - val_accuracy: 0.9771 - val_precision: 0.9699 - val_recall: 0.9612 - val_auc: 0.9985\n",
            "Epoch 2/100\n",
            "106/106 [==============================] - 27s 257ms/step - loss: 0.1770 - tp: 346337.0000 - fp: 7221.0000 - tn: 701529.0000 - fn: 8038.0000 - accuracy: 0.6599 - precision: 0.9796 - recall: 0.9773 - auc: 0.9992 - val_loss: 0.0351 - val_tp: 102538.0000 - val_fp: 1619.0000 - val_tn: 206911.0000 - val_fn: 1727.0000 - val_accuracy: 0.9893 - val_precision: 0.9845 - val_recall: 0.9834 - val_auc: 0.9996\n",
            "Epoch 3/100\n",
            "106/106 [==============================] - 27s 256ms/step - loss: 0.0637 - tp: 352067.0000 - fp: 2243.0000 - tn: 706507.0000 - fn: 2308.0000 - accuracy: 0.6646 - precision: 0.9937 - recall: 0.9935 - auc: 0.9998 - val_loss: 0.0347 - val_tp: 102730.0000 - val_fp: 1487.0000 - val_tn: 207043.0000 - val_fn: 1535.0000 - val_accuracy: 0.9903 - val_precision: 0.9857 - val_recall: 0.9853 - val_auc: 0.9993\n",
            "Epoch 4/100\n",
            "106/106 [==============================] - 27s 257ms/step - loss: 0.0321 - tp: 353125.0000 - fp: 1231.0000 - tn: 707519.0000 - fn: 1250.0000 - accuracy: 0.6655 - precision: 0.9965 - recall: 0.9965 - auc: 0.9999 - val_loss: 0.0368 - val_tp: 102833.0000 - val_fp: 1401.0000 - val_tn: 207129.0000 - val_fn: 1432.0000 - val_accuracy: 0.9909 - val_precision: 0.9866 - val_recall: 0.9863 - val_auc: 0.9989\n",
            "Epoch 5/100\n",
            "106/106 [==============================] - 27s 258ms/step - loss: 0.0209 - tp: 353576.0000 - fp: 790.0000 - tn: 707960.0000 - fn: 799.0000 - accuracy: 0.6659 - precision: 0.9978 - recall: 0.9977 - auc: 0.9999 - val_loss: 0.0410 - val_tp: 102774.0000 - val_fp: 1472.0000 - val_tn: 207058.0000 - val_fn: 1491.0000 - val_accuracy: 0.9905 - val_precision: 0.9859 - val_recall: 0.9857 - val_auc: 0.9986\n",
            "Epoch 6/100\n",
            "106/106 [==============================] - 27s 256ms/step - loss: 0.0159 - tp: 353780.0000 - fp: 590.0000 - tn: 708160.0000 - fn: 595.0000 - accuracy: 0.6661 - precision: 0.9983 - recall: 0.9983 - auc: 0.9999 - val_loss: 0.0438 - val_tp: 102840.0000 - val_fp: 1402.0000 - val_tn: 207128.0000 - val_fn: 1425.0000 - val_accuracy: 0.9910 - val_precision: 0.9866 - val_recall: 0.9863 - val_auc: 0.9983\n",
            "Epoch 7/100\n",
            "106/106 [==============================] - 27s 259ms/step - loss: 0.0118 - tp: 353948.0000 - fp: 426.0000 - tn: 708324.0000 - fn: 427.0000 - accuracy: 0.6663 - precision: 0.9988 - recall: 0.9988 - auc: 0.9999 - val_loss: 0.0464 - val_tp: 102803.0000 - val_fp: 1451.0000 - val_tn: 207079.0000 - val_fn: 1462.0000 - val_accuracy: 0.9907 - val_precision: 0.9861 - val_recall: 0.9860 - val_auc: 0.9981\n",
            "Epoch 8/100\n",
            "106/106 [==============================] - 27s 258ms/step - loss: 0.0101 - tp: 354028.0000 - fp: 342.0000 - tn: 708408.0000 - fn: 347.0000 - accuracy: 0.6663 - precision: 0.9990 - recall: 0.9990 - auc: 0.9999 - val_loss: 0.0493 - val_tp: 102709.0000 - val_fp: 1544.0000 - val_tn: 206986.0000 - val_fn: 1556.0000 - val_accuracy: 0.9901 - val_precision: 0.9852 - val_recall: 0.9851 - val_auc: 0.9980\n",
            "Epoch 9/100\n",
            "106/106 [==============================] - 28s 264ms/step - loss: 0.0084 - tp: 354100.0000 - fp: 272.0000 - tn: 708478.0000 - fn: 275.0000 - accuracy: 0.6664 - precision: 0.9992 - recall: 0.9992 - auc: 0.9999 - val_loss: 0.0501 - val_tp: 102837.0000 - val_fp: 1422.0000 - val_tn: 207108.0000 - val_fn: 1428.0000 - val_accuracy: 0.9909 - val_precision: 0.9864 - val_recall: 0.9863 - val_auc: 0.9978\n",
            "Epoch 10/100\n",
            "106/106 [==============================] - 27s 257ms/step - loss: 0.0071 - tp: 354135.0000 - fp: 236.0000 - tn: 708514.0000 - fn: 240.0000 - accuracy: 0.6664 - precision: 0.9993 - recall: 0.9993 - auc: 0.9999 - val_loss: 0.0508 - val_tp: 102824.0000 - val_fp: 1437.0000 - val_tn: 207093.0000 - val_fn: 1441.0000 - val_accuracy: 0.9908 - val_precision: 0.9862 - val_recall: 0.9862 - val_auc: 0.9977\n",
            "Epoch 11/100\n",
            "106/106 [==============================] - 27s 258ms/step - loss: 0.0083 - tp: 354124.0000 - fp: 249.0000 - tn: 708501.0000 - fn: 251.0000 - accuracy: 0.6664 - precision: 0.9993 - recall: 0.9993 - auc: 0.9999 - val_loss: 0.0568 - val_tp: 102584.0000 - val_fp: 1659.0000 - val_tn: 206871.0000 - val_fn: 1681.0000 - val_accuracy: 0.9893 - val_precision: 0.9841 - val_recall: 0.9839 - val_auc: 0.9976\n",
            "Epoch 12/100\n",
            "106/106 [==============================] - ETA: 0s - loss: 0.0058 - tp: 354187.0000 - fp: 185.0000 - tn: 708565.0000 - fn: 188.0000 - accuracy: 0.6665 - precision: 0.9995 - recall: 0.9995 - auc: 0.9999Restoring model weights from the end of the best epoch.\n",
            "106/106 [==============================] - 27s 256ms/step - loss: 0.0058 - tp: 354187.0000 - fp: 185.0000 - tn: 708565.0000 - fn: 188.0000 - accuracy: 0.6665 - precision: 0.9995 - recall: 0.9995 - auc: 0.9999 - val_loss: 0.0536 - val_tp: 102884.0000 - val_fp: 1375.0000 - val_tn: 207155.0000 - val_fn: 1381.0000 - val_accuracy: 0.9912 - val_precision: 0.9868 - val_recall: 0.9868 - val_auc: 0.9974\n",
            "Epoch 00012: early stopping\n"
          ],
          "name": "stdout"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "<tensorflow.python.keras.callbacks.History at 0x7ffb764d8ac8>"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 39
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "RVjq5NhDmeyE"
      },
      "source": [
        "# Model evaluation"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "qgYuRCyXuBj-"
      },
      "source": [
        "### Data processing"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "82ACDvi1tIJl"
      },
      "source": [
        "Predictions on the development set and labels distribution in the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "AzEn-s_B8_Z2",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "7809bafc-233d-4fa0-d24e-73f3e3dd29ab"
      },
      "source": [
        "# use argmax to figure out the class with highest probability per token\n",
        "preds = np.argmax(model.predict(dev_seqs_padded), axis=-1)\n",
        "flat_preds = [p for pred in preds for p in pred]\n",
        "print(Counter(flat_preds))"
      ],
      "execution_count": 40,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({2: 88890, 1: 13430, 0: 1945})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "be-NC5AmtVAB"
      },
      "source": [
        "We remove padding elements from sequences to retrieve the original sequence length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "VD3-NK606v_I",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 373
        },
        "outputId": "932cb1ef-e81f-4d84-fc3d-a85e50952abe"
      },
      "source": [
        "# start a new column for the model predictions\n",
        "dev_seqs['prediction'] = ''\n",
        "\n",
        "# for each text: get original sequence length and trim predictions accordingly\n",
        "# (_trim_ because we know that our seq length is longer than the longest seq in dev)\n",
        "for i in dev_seqs.index:\n",
        "  this_seq_length = len(dev_seqs['token'][i])\n",
        "  dev_seqs['prediction'][i] = preds[i][:this_seq_length].astype(int)\n",
        "\n",
        "dev_seqs.head()"
      ],
      "execution_count": 41,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[stabilized, approach, or, not, ?, that, ´, s,...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[14801.0, 10361.0, 414.0, 556.0, 131.0, 1740.0...</td>\n",
              "      <td>[0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[you, should, ', ve, stayed, on, redondo, beac...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 0.0, 0.0, 0.0, ...</td>\n",
              "      <td>[151.0, 1018.0, 573.0, 12927.0, 9346.0, 137.0,...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[all, i, ', ve, been, doing, is, binge, watchi...</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[405.0, 7.0, 573.0, 12927.0, 90.0, 848.0, 52.0...</td>\n",
              "      <td>[1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0]</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[wow, emma, and, kaite, is, so, very, cute, an...</td>\n",
              "      <td>[1.0, 0.0, 1.0, 0.0, 1.0, 1.0, 1.0, 1.0, 1.0, ...</td>\n",
              "      <td>[4777.0, 14801.0, 113.0, 14801.0, 52.0, 79.0, ...</td>\n",
              "      <td>[1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[this, is, so, good]</td>\n",
              "      <td>[1.0, 1.0, 1.0, 1.0]</td>\n",
              "      <td>[2239.0, 1567.0, 1089.0, 9176.0]</td>\n",
              "      <td>[1, 1, 1, 1]</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                         prediction\n",
              "0             0  ...               [0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 1, 1]\n",
              "1             1  ...  [1, 1, 1, 1, 1, 1, 0, 0, 0, 1, 1, 1, 1, 1, 0, ...\n",
              "2             2  ...            [1, 1, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0]\n",
              "3             3  ...  [1, 0, 1, 0, 1, 1, 1, 1, 1, 1, 0, 0, 0, 0, 0, ...\n",
              "4             4  ...                                       [1, 1, 1, 1]\n",
              "\n",
              "[5 rows x 5 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 41
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "YiByy8IFtgDg"
      },
      "source": [
        "We convert data into their original table format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "e9qny7OsnkBl",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "ee3933ba-78e7-49db-a24e-549c29e6aa4d"
      },
      "source": [
        "# use sequence number as the index and apply pandas explode to all other columns\n",
        "dev_long = dev_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "dev_long.head()"
      ],
      "execution_count": 42,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>stabilized</td>\n",
              "      <td>1</td>\n",
              "      <td>14801</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>approach</td>\n",
              "      <td>1</td>\n",
              "      <td>10361</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>or</td>\n",
              "      <td>1</td>\n",
              "      <td>414</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>not</td>\n",
              "      <td>1</td>\n",
              "      <td>556</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>?</td>\n",
              "      <td>1</td>\n",
              "      <td>131</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num       token bio_only token_indices prediction\n",
              "0             0  stabilized        1         14801          0\n",
              "1             0    approach        1         10361          1\n",
              "2             0          or        1           414          1\n",
              "3             0         not        1           556          1\n",
              "4             0           ?        1           131          1"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 42
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "fhSoNXYMtkXt"
      },
      "source": [
        "We convert NER labels into the BIO (BIO2) tagging scheme, because it is the one used to label the development and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mXbybZ6wzUB-",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "9b256171-5b4d-4e88-a60c-6d43b4816e73"
      },
      "source": [
        "if (ner_scheme == 'IO'):\n",
        "  bio_labs = io_to_bio([reverse_io(b) for b in dev_long['bio_only']])\n",
        "  dev_long['bio_only'] = bio_labs\n",
        "  pred_labs = io_to_bio([reverse_io(b) for b in dev_long['prediction']])\n",
        "elif (ner_scheme == 'BIO'):\n",
        "  bio_labs = [reverse_bio(b) for b in dev_long['bio_only']]\n",
        "  dev_long['bio_only'] = bio_labs\n",
        "  pred_labs = [reverse_bio(b) for b in dev_long['prediction']]\n",
        "  pred_labs = correct_preds(pred_labs)\n",
        "elif (ner_scheme == 'BIO1'):\n",
        "  bio_labs = bio1_to_bio([reverse_bio1(b) for b in dev_long['bio_only']])\n",
        "  dev_long['bio_only'] = bio_labs\n",
        "  pred_labs = bio1_to_bio([reverse_bio1(b) for b in dev_long['prediction']])\n",
        "elif (ner_scheme == 'IOE1'):\n",
        "  bio_labs = ioe1_to_bio([reverse_ioe1(b) for b in dev_long['bio_only']])\n",
        "  dev_long['bio_only'] = bio_labs\n",
        "  pred_labs = ioe1_to_bio([reverse_ioe1(b) for b in dev_long['prediction']])\n",
        "elif (ner_scheme == 'IOE2'):\n",
        "  bio_labs = ioe2_to_bio([reverse_ioe2(b) for b in dev_long['bio_only']])\n",
        "  dev_long['bio_only'] = bio_labs\n",
        "  pred_labs = ioe2_to_bio([reverse_ioe2(b) for b in dev_long['prediction']])\n",
        "else:\n",
        "  bio_labs = bilou_to_bio([reverse_bilou(b) for b in dev_long['bio_only']])\n",
        "  dev_long['bio_only'] = bio_labs\n",
        "  pred_labs = bilou_to_bio([reverse_bilou(b) for b in dev_long['prediction']])\n",
        "\n",
        "dev_long['prediction'] = pred_labs\n",
        "\n",
        "dev_long.head()\n",
        "dev_long.prediction.value_counts()"
      ],
      "execution_count": 43,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/plain": [
              "O    13430\n",
              "B     1314\n",
              "I      638\n",
              "Name: prediction, dtype: int64"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 43
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BoTYgds1uEcA"
      },
      "source": [
        "### Evaluation of predictions"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "lAgvOUCMmlC7"
      },
      "source": [
        "This function aims at computing the precision, recall and F1-score metrics of our model. We use entity-level measures in order to reward only once multi-token named-entities."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "FTxztRdUeoBw"
      },
      "source": [
        "# evaluation function\n",
        "def wnut_evaluate(txt):\n",
        "  '''row by row entity evaluation: we evaluate by whole named entities'''\n",
        "  tp = 0; fp = 0; fn = 0\n",
        "  in_entity = 0\n",
        "  for i in txt.index:\n",
        "    if txt['prediction'][i]=='B' and txt['bio_only'][i]=='B':\n",
        "      if in_entity==1:  # if there's a preceding named entity which didn't have intervening O...\n",
        "        tp += 1  # count a true positive\n",
        "      in_entity = 1  # start tracking this entity (don't count it until we know full span of entity)\n",
        "    elif txt['prediction'][i]=='B':\n",
        "      fp += 1  # if not a B in gold annotations, it's a false positive\n",
        "      in_entity = 0\n",
        "    elif txt['prediction'][i]=='I' and txt['bio_only'][i]=='I':\n",
        "      next  # correct entity continuation: do nothing\n",
        "    elif txt['prediction'][i]=='I' and txt['bio_only'][i]=='B':\n",
        "      fn += 1  # if a new entity should have begun, it's a false negative\n",
        "      in_entity = 0\n",
        "    elif txt['prediction'][i]=='I':  # if gold is O...\n",
        "      if in_entity==1:  # and if tracking an entity, then the span is too long\n",
        "        fp += 1  # it's a false positive\n",
        "      in_entity = 0\n",
        "    elif txt['prediction'][i]=='O':\n",
        "      if txt['bio_only'][i]=='B':\n",
        "        fn += 1  # false negative if there's B in gold but no predicted B\n",
        "        if in_entity==1:  # also check if there was a named entity in progress\n",
        "          tp += 1  # count a true positive\n",
        "      elif txt['bio_only'][i]=='I':\n",
        "        if in_entity==1:  # if this should have been a continued named entity, the span is too short\n",
        "          fn += 1  # count a false negative\n",
        "      elif txt['bio_only'][i]=='O':\n",
        "        if in_entity==1:  # if a named entity has ended in right place\n",
        "          tp += 1  # count a true positive\n",
        "      in_entity = 0\n",
        "\n",
        "  if in_entity==1:  # catch any final named entity\n",
        "    tp += 1\n",
        "\n",
        "  prec = tp / (tp+fp)\n",
        "  rec = tp / (tp+fn)\n",
        "  f1 = (2*(prec*rec)) / (prec+rec)\n",
        "  print('Sum of TP and FP = %i' % (tp+fp))\n",
        "  print('Sum of TP and FN = %i' % (tp+fn))\n",
        "  print('True positives = %i, False positives = %i, False negatives = %i' % (tp, fp, fn))\n",
        "  print('Precision = %.3f, Recall = %.3f, F1 = %.3f' % (prec, rec, f1))"
      ],
      "execution_count": 44,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "UM4r9Fy2Nl3e",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "3e398eeb-49e2-48ce-efde-2134f5a6c3d6"
      },
      "source": [
        "wnut_evaluate(dev_long)"
      ],
      "execution_count": 45,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sum of TP and FP = 1240\n",
            "Sum of TP and FN = 760\n",
            "True positives = 339, False positives = 901, False negatives = 421\n",
            "Precision = 0.273, Recall = 0.446, F1 = 0.339\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "2SKEbKhL3YIH"
      },
      "source": [
        "# Predictions on the test set"
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "M8Wki8L97sWM"
      },
      "source": [
        "### Predictions on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "mYMjG8KU0GdC"
      },
      "source": [
        "wnuttest = 'https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_clean_tagged.txt'\n",
        "test = pd.read_table(wnuttest, header=None, names=['token', 'upos'])\n",
        "\n",
        "test_copy = extract_features(test, istest=True)\n",
        "test_seqs = tokens2sequences(test_copy, True)"
      ],
      "execution_count": 46,
      "outputs": []
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "HeDTrHEiuxO8",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "2a674d62-c29a-4a71-d797-781ed13fcbcb"
      },
      "source": [
        "# now process the dev set in the same way: padding the tokens & labels, and one-hot encoding the labels\n",
        "test_seqs_padded = pad_sequences(test_seqs['token_indices'].tolist(), maxlen=seq_length,\n",
        "                                dtype='int32', padding='post', truncating='post', value=padtok)\n",
        "\n",
        "print('Dev set padded label sequence and one-hot encoding (first 10 tokens):')\n",
        "print(test_seqs.loc[2])"
      ],
      "execution_count": 47,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Dev set padded label sequence and one-hot encoding (first 10 tokens):\n",
            "sequence_num                                                     2\n",
            "token            [&, gt, ;, *, the, army, on, thursday, recover...\n",
            "token_indices    [14801.0, 14801.0, 1625.0, 1743.0, 191.0, 1480...\n",
            "Name: 2, dtype: object\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "51fKVMLNubY4"
      },
      "source": [
        "test_X = np.array(test_seqs_padded)"
      ],
      "execution_count": 48,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "JtpKiNduwL0J"
      },
      "source": [
        "Predictions on the test set and labels distribution in the predictions"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "yJ4770GiwL0J",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "62698ec8-ae76-4b3c-ccb5-2c5dd32f495e"
      },
      "source": [
        "# use argmax to figure out the class with highest probability per token\n",
        "preds_test = np.argmax(model.predict(test_seqs_padded), axis=-1)\n",
        "flat_preds_test = [p for pred in preds_test for p in pred]\n",
        "print(Counter(flat_preds_test))"
      ],
      "execution_count": 49,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Counter({2: 111427, 1: 19180, 0: 4108})\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "NUDe8cc6wL0J"
      },
      "source": [
        "We remove padding elements from sequences to retrieve the original sequence length."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "2W_KFBXowL0J",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 390
        },
        "outputId": "e37077fa-1011-4830-e5ea-95ef3d915d1b"
      },
      "source": [
        "# start a new column for the model predictions\n",
        "test_seqs['prediction'] = ''\n",
        "\n",
        "# for each text: get original sequence length and trim predictions accordingly\n",
        "# (_trim_ because we know that our seq length is longer than the longest seq in dev)\n",
        "for i in test_seqs.index:\n",
        "  this_seq_length = len(test_seqs['token'][i])\n",
        "  test_seqs['prediction'][i] = preds_test[i][:this_seq_length].astype(int)\n",
        "\n",
        "test_seqs.head()"
      ],
      "execution_count": 50,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "/usr/local/lib/python3.6/dist-packages/ipykernel_launcher.py:8: SettingWithCopyWarning: \n",
            "A value is trying to be set on a copy of a slice from a DataFrame\n",
            "\n",
            "See the caveats in the documentation: https://pandas.pydata.org/pandas-docs/stable/user_guide/indexing.html#returning-a-view-versus-a-copy\n",
            "  \n"
          ],
          "name": "stderr"
        },
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>[&amp;, gt, ;, *, the, soldier, was, killed, when,...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 191.0, 1480...</td>\n",
              "      <td>[0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>1</td>\n",
              "      <td>[&amp;, gt, ;, *, police, last, week, evacuated, 8...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 14801.0, 23...</td>\n",
              "      <td>[0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>2</td>\n",
              "      <td>[&amp;, gt, ;, *, the, army, on, thursday, recover...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 191.0, 1480...</td>\n",
              "      <td>[0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>3</td>\n",
              "      <td>[&amp;, gt, ;, *, the, four, civilians, killed, in...</td>\n",
              "      <td>[14801.0, 14801.0, 1625.0, 1743.0, 191.0, 4012...</td>\n",
              "      <td>[0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>4</td>\n",
              "      <td>[the, bodies, of, the, soldiers, were, recover...</td>\n",
              "      <td>[191.0, 14801.0, 45.0, 3.0, 14801.0, 225.0, 14...</td>\n",
              "      <td>[0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, ...</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num  ...                                         prediction\n",
              "0             0  ...  [0, 0, 1, 1, 0, 0, 1, 1, 1, 1, 0, 1, 1, 0, 0, ...\n",
              "1             1  ...  [0, 0, 1, 1, 1, 1, 1, 0, 1, 0, 1, 0, 0, 1, 1, ...\n",
              "2             2  ...  [0, 0, 1, 1, 0, 0, 1, 1, 0, 1, 0, 1, 1, 1, 1, ...\n",
              "3             3  ...  [0, 0, 1, 1, 0, 1, 0, 0, 1, 1, 1, 1, 1, 0, 0, ...\n",
              "4             4  ...  [0, 0, 1, 1, 0, 1, 0, 1, 1, 0, 0, 1, 1, 0, 0, ...\n",
              "\n",
              "[5 rows x 4 columns]"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 50
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "7h4VPe7VwL0K"
      },
      "source": [
        "We convert data into their original table format."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "8n3XEX4NwL0K",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "b89f16ee-3cb5-41db-bd03-d49520f7464b"
      },
      "source": [
        "# use sequence number as the index and apply pandas explode to all other columns\n",
        "test_long = test_seqs.set_index('sequence_num').apply(pd.Series.explode).reset_index()\n",
        "test_long.head()"
      ],
      "execution_count": 51,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>sequence_num</th>\n",
              "      <th>token</th>\n",
              "      <th>token_indices</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>0</td>\n",
              "      <td>&amp;</td>\n",
              "      <td>14801</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>0</td>\n",
              "      <td>gt</td>\n",
              "      <td>14801</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>0</td>\n",
              "      <td>;</td>\n",
              "      <td>1625</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>0</td>\n",
              "      <td>*</td>\n",
              "      <td>1743</td>\n",
              "      <td>1</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>0</td>\n",
              "      <td>the</td>\n",
              "      <td>191</td>\n",
              "      <td>0</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "   sequence_num token token_indices prediction\n",
              "0             0     &         14801          0\n",
              "1             0    gt         14801          0\n",
              "2             0     ;          1625          1\n",
              "3             0     *          1743          1\n",
              "4             0   the           191          0"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 51
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "BU5mI5EswL0K"
      },
      "source": [
        "We convert NER labels into the BIO (BIO2) tagging scheme, because it is the one used to label the development and test sets.\n"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "Q_HwN57NwL0K",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "66862e28-669e-4bc6-8eba-90aefb008a3c"
      },
      "source": [
        "if (ner_scheme == 'IO'):\n",
        "  test_pred_labs = io_to_bio([reverse_io(b) for b in test_long['prediction']])\n",
        "elif (ner_scheme == 'BIO'):\n",
        "  test_pred_labs = [reverse_bio(b) for b in test_long['prediction']]\n",
        "  test_pred_labs = correct_preds(test_pred_labs)\n",
        "elif (ner_scheme == 'BIO1'):\n",
        "  test_pred_labs = bio1_to_bio([reverse_bio1(b) for b in dev_long['prediction']])\n",
        "elif (ner_scheme == 'IOE1'):\n",
        "  test_pred_labs = ioe1_to_bio([reverse_ioe1(b) for b in dev_long['prediction']])\n",
        "elif (ner_scheme == 'IOE2'):\n",
        "  test_pred_labs = ioe2_to_bio([reverse_ioe2(b) for b in test_long['prediction']])\n",
        "else:\n",
        "  test_pred_labs = bilou_to_bio([reverse_bilou(b) for b in test_long['prediction']])\n",
        "\n",
        "test_long['prediction'] = test_pred_labs\n",
        "\n",
        "print(test_long.head())\n",
        "print(test_long.prediction.value_counts())"
      ],
      "execution_count": 52,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "   sequence_num token token_indices prediction\n",
            "0             0     &         14801          B\n",
            "1             0    gt         14801          I\n",
            "2             0     ;          1625          O\n",
            "3             0     *          1743          O\n",
            "4             0   the           191          B\n",
            "O    19180\n",
            "B     2764\n",
            "I     1379\n",
            "Name: prediction, dtype: int64\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "-bsj6j8C7-vN"
      },
      "source": [
        "We add our predictions to the initial test dataset."
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "k3bebukX46dV",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "e42d8a68-2128-46da-9e38-9cbb674d3d42"
      },
      "source": [
        "j = 0\n",
        "test['prediction'] = None\n",
        "for i in range(len(test)):\n",
        "  if str(test.token[i]) != \"nan\":  # all NaN lines have been removed in test_long\n",
        "    test.loc[i,'prediction'] = test_long.loc[j,'prediction']\n",
        "    j += 1\n",
        "print(test)"
      ],
      "execution_count": 53,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "       token   upos prediction\n",
            "0          &  CCONJ          B\n",
            "1         gt      X          I\n",
            "2          ;  PUNCT          O\n",
            "3          *  PUNCT          O\n",
            "4        The    DET          B\n",
            "...      ...    ...        ...\n",
            "24601   this   PRON          O\n",
            "24602  dress   NOUN          O\n",
            "24603   code   NOUN          O\n",
            "24604      😂    SYM          O\n",
            "24605    NaN    NaN       None\n",
            "\n",
            "[24606 rows x 3 columns]\n"
          ],
          "name": "stdout"
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "LD2tjGjoLoYN"
      },
      "source": [
        "test.to_csv(\"/content/drive/My Drive/Cambridge/NER/test_preds.csv\", index=False)"
      ],
      "execution_count": 54,
      "outputs": []
    },
    {
      "cell_type": "markdown",
      "metadata": {
        "id": "WJsRDN9m7v41"
      },
      "source": [
        "### Evaluation on the test set"
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "aM44UhjF5smi",
        "colab": {
          "base_uri": "https://localhost:8080/",
          "height": 197
        },
        "outputId": "25d80209-f2e3-4959-a192-762b6f43e5ec"
      },
      "source": [
        "test_tag_file = \"https://storage.googleapis.com/wnut-2017_ner-shared-task/wnut17test_annotated_clean_tagged.txt\"\n",
        "test_tag = pd.read_table(test_tag_file, header=None, names=['token','bio','bio_only','upos'])\n",
        "test_tag['prediction'] = test['prediction']\n",
        "test_tag.head()"
      ],
      "execution_count": 55,
      "outputs": [
        {
          "output_type": "execute_result",
          "data": {
            "text/html": [
              "<div>\n",
              "<style scoped>\n",
              "    .dataframe tbody tr th:only-of-type {\n",
              "        vertical-align: middle;\n",
              "    }\n",
              "\n",
              "    .dataframe tbody tr th {\n",
              "        vertical-align: top;\n",
              "    }\n",
              "\n",
              "    .dataframe thead th {\n",
              "        text-align: right;\n",
              "    }\n",
              "</style>\n",
              "<table border=\"1\" class=\"dataframe\">\n",
              "  <thead>\n",
              "    <tr style=\"text-align: right;\">\n",
              "      <th></th>\n",
              "      <th>token</th>\n",
              "      <th>bio</th>\n",
              "      <th>bio_only</th>\n",
              "      <th>upos</th>\n",
              "      <th>prediction</th>\n",
              "    </tr>\n",
              "  </thead>\n",
              "  <tbody>\n",
              "    <tr>\n",
              "      <th>0</th>\n",
              "      <td>&amp;</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>CCONJ</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>1</th>\n",
              "      <td>gt</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>X</td>\n",
              "      <td>I</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>2</th>\n",
              "      <td>;</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>3</th>\n",
              "      <td>*</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>PUNCT</td>\n",
              "      <td>O</td>\n",
              "    </tr>\n",
              "    <tr>\n",
              "      <th>4</th>\n",
              "      <td>The</td>\n",
              "      <td>O</td>\n",
              "      <td>O</td>\n",
              "      <td>DET</td>\n",
              "      <td>B</td>\n",
              "    </tr>\n",
              "  </tbody>\n",
              "</table>\n",
              "</div>"
            ],
            "text/plain": [
              "  token bio bio_only   upos prediction\n",
              "0     &   O        O  CCONJ          B\n",
              "1    gt   O        O      X          I\n",
              "2     ;   O        O  PUNCT          O\n",
              "3     *   O        O  PUNCT          O\n",
              "4   The   O        O    DET          B"
            ]
          },
          "metadata": {
            "tags": []
          },
          "execution_count": 55
        }
      ]
    },
    {
      "cell_type": "code",
      "metadata": {
        "id": "0Xq902jq7YIo",
        "colab": {
          "base_uri": "https://localhost:8080/"
        },
        "outputId": "5d693a00-3d59-4a75-a3bb-64ff168e1a46"
      },
      "source": [
        "wnut_evaluate(test_tag)"
      ],
      "execution_count": 56,
      "outputs": [
        {
          "output_type": "stream",
          "text": [
            "Sum of TP and FP = 2682\n",
            "Sum of TP and FN = 951\n",
            "True positives = 479, False positives = 2203, False negatives = 472\n",
            "Precision = 0.179, Recall = 0.504, F1 = 0.264\n"
          ],
          "name": "stdout"
        }
      ]
    }
  ]
}